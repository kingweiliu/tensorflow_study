{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liujingwei/pythonenv/basic_seq2seq/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bsaqq', 'npy', 'lbwuj', 'bqv', 'kial']\n",
      "['abqqs', 'npy', 'bjluw', 'bqv', 'aikl']\n",
      "{'<PAD>': 0, '<UNK>': 1, '<GO>': 2, '<EOS>': 3, 'q': 4, 'd': 5, 'e': 6, 'z': 7, 'k': 8, 'c': 9, 'r': 10, 'l': 11, 'b': 12, 'y': 13, 'x': 14, 'j': 15, 'w': 16, 'a': 17, 'v': 18, 'p': 19, 'n': 20, 'f': 21, 'h': 22, 's': 23, 'u': 24, 't': 25, 'g': 26, 'o': 27, 'i': 28, 'm': 29}\n",
      "{'<PAD>': 0, '<UNK>': 1, '<GO>': 2, '<EOS>': 3, 'q': 4, 'd': 5, 'e': 6, 'z': 7, 'k': 8, 'c': 9, 'r': 10, 'l': 11, 'b': 12, 'y': 13, 'x': 14, 'j': 15, 'w': 16, 'a': 17, 'v': 18, 'p': 19, 'n': 20, 'f': 21, 'h': 22, 's': 23, 'u': 24, 't': 25, 'g': 26, 'o': 27, 'i': 28, 'm': 29}\n"
     ]
    }
   ],
   "source": [
    "def read_file(filename):\n",
    "    with tf.gfile.Open(filename) as f:\n",
    "        return [x.strip() for x in f]\n",
    "    \n",
    "def extract_char_vocab(filedata):\n",
    "    special_words = ['<PAD>', '<UNK>',  '<GO>', '<EOS>']\n",
    "    s = list(set([x for line in filedata for x in line]))\n",
    "    index2char = {index:chr for index, chr in enumerate(special_words + s)}\n",
    "    char2index = {chr:index for index, chr in index2char.items()}\n",
    "    return char2index, index2char\n",
    "source_filedata = read_file(\"data/letters_source.txt\")\n",
    "print(source_filedata[:5])\n",
    "source_char2index, source_index2char = extract_char_vocab(source_filedata)\n",
    "target_filedata= read_file(\"data/letters_target.txt\")\n",
    "print(target_filedata[:5])\n",
    "target_char2index, target_index2char = extract_char_vocab(target_filedata)\n",
    "\n",
    "print(source_char2index)\n",
    "print(target_char2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 23, 17, 4, 4], [20, 19, 13], [11, 12, 16, 24, 15], [12, 4, 18], [8, 28, 17, 11]]\n",
      "[[17, 12, 4, 4, 23], [20, 19, 13], [12, 15, 11, 24, 16], [12, 4, 18], [17, 28, 8, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 将输入变化为数字符号\n",
    "def input_2_ids(inputdata, vocab_char2index):\n",
    "    return [[vocab_char2index.get(chr, vocab_char2index.get(\"<UNK>\")) for chr in sentence] for sentence in inputdata]\n",
    "source_input_ids = input_2_ids(source_filedata, source_char2index)\n",
    "target_input_ids = input_2_ids(target_filedata, target_char2index)\n",
    "print(source_input_ids[:5])\n",
    "print(target_input_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = \"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = \"target\")\n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name = \"target_sequence_length\")\n",
    "    max_target_length = tf.reduce_max(target_sequence_length, name = \"max_target_len\")\n",
    "    source_sequence_length = tf.placeholder(tf.int32, [None], name = \"source_sequence_length\")\n",
    "    max_source_length = tf.reduce_max(target_sequence_length, name = \"max_source_len\")\n",
    "    return inputs, targets, target_sequence_length, max_target_length, source_sequence_length, max_source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_layer(inputdata, rnn_size, num_layers, source_sequence_length, source_vocab_size, encoder_embedding_size):\n",
    "    input_embedding = tf.contrib.layers.embed_sequence(inputdata, vocab_size = source_vocab_size, embed_dim = encoder_embedding_size)\n",
    "    rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for x in range(num_layers)])\n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(rnn_cell, input_embedding, sequence_length = source_sequence_length, dtype=tf.float32 )\n",
    "    return encoder_output, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_target_input(targetdata, vocab_to_int, batch_size):\n",
    "    no_ending = tf.strided_slice(targetdata, [0,0], [batch_size, -1], [1,1])\n",
    "    decode_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int[\"<GO>\"]), no_ending], 1)\n",
    "    return decode_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(target_char2index, decoding_embedding_size, rnn_size, num_layers, target_sequence_length, max_target_length, \n",
    "                   encoder_state, decoder_input):\n",
    "    # 1. embedding \n",
    "    target_vocab_size = len(target_char2index)\n",
    "    decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "    \n",
    "    # 2. 构造Decoder中的rnn\n",
    "    rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for i in range(num_layers)])\n",
    "    \n",
    "    # 3. Output 全连接层\n",
    "    output_layer = tf.layers.Dense(target_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev = 0.1))\n",
    "    \n",
    "    # 4. Training \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        # helper\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_embed_input, sequence_length = target_sequence_length)\n",
    "        # 构造decoder\n",
    "        train_decode = tf.contrib.seq2seq.BasicDecoder(rnn_cell, training_helper, encoder_state, output_layer)\n",
    "        training_decode_output, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decode, impute_finished = True,maximum_iterations=max_target_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse = True):\n",
    "        start_tokens = tf.tile(tf.constant([target_char2index['<GO>']], dtype = tf.int32), [batch_size], name = \"start_tokens\")\n",
    "        predict_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings, start_tokens, target_char2index[\"<EOS>\"])\n",
    "        predict_decode = tf.contrib.seq2seq.BasicDecoder(rnn_cell, predict_helper, encoder_state, output_layer)\n",
    "        predict_decode_output, _, _ = tf.contrib.seq2seq.dynamic_decode(predict_decode, impute_finished = True, maximum_iterations=max_target_length)\n",
    "        \n",
    "    return training_decode_output, predict_decode_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, max_target_length, source_sequence_length, \n",
    "                  source_vocab_size, target_vocab_size, encoder_embedding_size, decoder_embedding_size, \n",
    "                 rnn_size, num_layers):\n",
    "    _, encoder_state = get_encoder_layer(input_data, rnn_size, num_layers, source_sequence_length, source_vocab_size, encoder_embedding_size)\n",
    "    decoder_input = process_target_input(targets, target_char2index, batch_size)\n",
    "    training_decode_output, predict_decode_output = decoding_layer(target_char2index, decoder_embedding_size, rnn_size, num_layers, \n",
    "                                                                   target_sequence_length, max_target_length, \n",
    "                                                                   encoder_state, decoder_input)\n",
    "    return training_decode_output, predict_decode_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "# Number of Epochs\n",
    "epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs, targets, target_sequence_length, max_target_length, source_sequence_length, max_source_length = get_inputs()\n",
    "    training_decoder_output, predicting_decoder_output = seq2seq_model(inputs, targets, None, target_sequence_length, max_target_length, \n",
    "                                                                  source_sequence_length, len(source_char2index), len(target_char2index),\n",
    "                                                                 encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers)\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_length, dtype=tf.float32, name='masks')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(0.05)\n",
    "        \n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence_len - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    for batch_i in range(0, len(sources)// batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i : start_i + batch_size]\n",
    "        targets_batch = targets[start_i : start_i + batch_size]\n",
    "        pad_sources_batch = pad_sentence_batch(sources_batch, source_pad_int)\n",
    "        pad_targets_batch = pad_sentence_batch(targets_batch, target_pad_int)\n",
    "        \n",
    "        targets_lengths = []\n",
    "        for target in targets_batch:\n",
    "            targets_lengths.append(len(target))\n",
    "        \n",
    "        sources_lengths = []\n",
    "        for source in sources_batch:\n",
    "            sources_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_sources_batch, sources_lengths, pad_targets_batch, targets_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/60 Batch    0/77 - Training loss: 3.403 - Valid loss: 3.293\n",
      "Epoch   1/60 Batch   50/77 - Training loss: 1.962 - Valid loss: 1.933\n",
      "Epoch   2/60 Batch    0/77 - Training loss: 1.430 - Valid loss: 1.406\n",
      "Epoch   2/60 Batch   50/77 - Training loss: 0.699 - Valid loss: 0.638\n",
      "Epoch   3/60 Batch    0/77 - Training loss: 0.460 - Valid loss: 0.428\n",
      "Epoch   3/60 Batch   50/77 - Training loss: 0.302 - Valid loss: 0.292\n",
      "Epoch   4/60 Batch    0/77 - Training loss: 0.230 - Valid loss: 0.230\n",
      "Epoch   4/60 Batch   50/77 - Training loss: 0.194 - Valid loss: 0.171\n",
      "Epoch   5/60 Batch    0/77 - Training loss: 0.116 - Valid loss: 0.117\n",
      "Epoch   5/60 Batch   50/77 - Training loss: 0.116 - Valid loss: 0.101\n",
      "Epoch   6/60 Batch    0/77 - Training loss: 0.112 - Valid loss: 0.110\n",
      "Epoch   6/60 Batch   50/77 - Training loss: 0.079 - Valid loss: 0.073\n",
      "Epoch   7/60 Batch    0/77 - Training loss: 0.065 - Valid loss: 0.072\n",
      "Epoch   7/60 Batch   50/77 - Training loss: 0.066 - Valid loss: 0.065\n",
      "Epoch   8/60 Batch    0/77 - Training loss: 0.045 - Valid loss: 0.096\n",
      "Epoch   8/60 Batch   50/77 - Training loss: 0.048 - Valid loss: 0.051\n",
      "Epoch   9/60 Batch    0/77 - Training loss: 0.099 - Valid loss: 0.057\n",
      "Epoch   9/60 Batch   50/77 - Training loss: 0.054 - Valid loss: 0.059\n",
      "Epoch  10/60 Batch    0/77 - Training loss: 0.037 - Valid loss: 0.059\n",
      "Epoch  10/60 Batch   50/77 - Training loss: 0.030 - Valid loss: 0.039\n",
      "Epoch  11/60 Batch    0/77 - Training loss: 0.033 - Valid loss: 0.044\n",
      "Epoch  11/60 Batch   50/77 - Training loss: 0.020 - Valid loss: 0.046\n",
      "Epoch  12/60 Batch    0/77 - Training loss: 0.038 - Valid loss: 0.050\n",
      "Epoch  12/60 Batch   50/77 - Training loss: 0.020 - Valid loss: 0.050\n",
      "Epoch  13/60 Batch    0/77 - Training loss: 0.025 - Valid loss: 0.033\n",
      "Epoch  13/60 Batch   50/77 - Training loss: 0.030 - Valid loss: 0.045\n",
      "Epoch  14/60 Batch    0/77 - Training loss: 0.020 - Valid loss: 0.050\n",
      "Epoch  14/60 Batch   50/77 - Training loss: 0.022 - Valid loss: 0.052\n",
      "Epoch  15/60 Batch    0/77 - Training loss: 0.030 - Valid loss: 0.038\n",
      "Epoch  15/60 Batch   50/77 - Training loss: 0.024 - Valid loss: 0.045\n",
      "Epoch  16/60 Batch    0/77 - Training loss: 0.024 - Valid loss: 0.054\n",
      "Epoch  16/60 Batch   50/77 - Training loss: 0.018 - Valid loss: 0.034\n",
      "Epoch  17/60 Batch    0/77 - Training loss: 0.020 - Valid loss: 0.038\n",
      "Epoch  17/60 Batch   50/77 - Training loss: 0.021 - Valid loss: 0.038\n",
      "Epoch  18/60 Batch    0/77 - Training loss: 0.022 - Valid loss: 0.026\n",
      "Epoch  18/60 Batch   50/77 - Training loss: 0.035 - Valid loss: 0.042\n",
      "Epoch  19/60 Batch    0/77 - Training loss: 0.019 - Valid loss: 0.039\n",
      "Epoch  19/60 Batch   50/77 - Training loss: 0.017 - Valid loss: 0.019\n",
      "Epoch  20/60 Batch    0/77 - Training loss: 0.021 - Valid loss: 0.047\n",
      "Epoch  20/60 Batch   50/77 - Training loss: 0.016 - Valid loss: 0.025\n",
      "Epoch  21/60 Batch    0/77 - Training loss: 0.020 - Valid loss: 0.056\n",
      "Epoch  21/60 Batch   50/77 - Training loss: 0.037 - Valid loss: 0.037\n",
      "Epoch  22/60 Batch    0/77 - Training loss: 0.031 - Valid loss: 0.028\n",
      "Epoch  22/60 Batch   50/77 - Training loss: 0.170 - Valid loss: 0.211\n",
      "Epoch  23/60 Batch    0/77 - Training loss: 0.070 - Valid loss: 0.064\n",
      "Epoch  23/60 Batch   50/77 - Training loss: 0.047 - Valid loss: 0.067\n",
      "Epoch  24/60 Batch    0/77 - Training loss: 0.038 - Valid loss: 0.100\n",
      "Epoch  24/60 Batch   50/77 - Training loss: 0.027 - Valid loss: 0.039\n",
      "Epoch  25/60 Batch    0/77 - Training loss: 0.059 - Valid loss: 0.042\n",
      "Epoch  25/60 Batch   50/77 - Training loss: 0.010 - Valid loss: 0.037\n",
      "Epoch  26/60 Batch    0/77 - Training loss: 0.022 - Valid loss: 0.038\n",
      "Epoch  26/60 Batch   50/77 - Training loss: 0.014 - Valid loss: 0.033\n",
      "Epoch  27/60 Batch    0/77 - Training loss: 0.026 - Valid loss: 0.034\n",
      "Epoch  27/60 Batch   50/77 - Training loss: 0.015 - Valid loss: 0.037\n",
      "Epoch  28/60 Batch    0/77 - Training loss: 0.014 - Valid loss: 0.019\n",
      "Epoch  28/60 Batch   50/77 - Training loss: 0.014 - Valid loss: 0.026\n",
      "Epoch  29/60 Batch    0/77 - Training loss: 0.009 - Valid loss: 0.024\n",
      "Epoch  29/60 Batch   50/77 - Training loss: 0.009 - Valid loss: 0.020\n",
      "Epoch  30/60 Batch    0/77 - Training loss: 0.005 - Valid loss: 0.020\n",
      "Epoch  30/60 Batch   50/77 - Training loss: 0.008 - Valid loss: 0.023\n",
      "Epoch  31/60 Batch    0/77 - Training loss: 0.004 - Valid loss: 0.014\n",
      "Epoch  31/60 Batch   50/77 - Training loss: 0.005 - Valid loss: 0.016\n",
      "Epoch  32/60 Batch    0/77 - Training loss: 0.005 - Valid loss: 0.015\n",
      "Epoch  32/60 Batch   50/77 - Training loss: 0.010 - Valid loss: 0.023\n",
      "Epoch  33/60 Batch    0/77 - Training loss: 0.026 - Valid loss: 0.020\n",
      "Epoch  33/60 Batch   50/77 - Training loss: 0.018 - Valid loss: 0.017\n",
      "Epoch  34/60 Batch    0/77 - Training loss: 0.015 - Valid loss: 0.023\n",
      "Epoch  34/60 Batch   50/77 - Training loss: 0.004 - Valid loss: 0.043\n",
      "Epoch  35/60 Batch    0/77 - Training loss: 0.008 - Valid loss: 0.038\n",
      "Epoch  35/60 Batch   50/77 - Training loss: 0.007 - Valid loss: 0.056\n",
      "Epoch  36/60 Batch    0/77 - Training loss: 0.008 - Valid loss: 0.036\n",
      "Epoch  36/60 Batch   50/77 - Training loss: 0.006 - Valid loss: 0.040\n",
      "Epoch  37/60 Batch    0/77 - Training loss: 0.010 - Valid loss: 0.041\n",
      "Epoch  37/60 Batch   50/77 - Training loss: 0.005 - Valid loss: 0.038\n",
      "Epoch  38/60 Batch    0/77 - Training loss: 0.022 - Valid loss: 0.049\n",
      "Epoch  38/60 Batch   50/77 - Training loss: 0.019 - Valid loss: 0.047\n",
      "Epoch  39/60 Batch    0/77 - Training loss: 0.025 - Valid loss: 0.060\n",
      "Epoch  39/60 Batch   50/77 - Training loss: 0.013 - Valid loss: 0.039\n",
      "Epoch  40/60 Batch    0/77 - Training loss: 0.019 - Valid loss: 0.049\n",
      "Epoch  40/60 Batch   50/77 - Training loss: 0.014 - Valid loss: 0.023\n",
      "Epoch  41/60 Batch    0/77 - Training loss: 0.045 - Valid loss: 0.023\n",
      "Epoch  41/60 Batch   50/77 - Training loss: 0.013 - Valid loss: 0.029\n",
      "Epoch  42/60 Batch    0/77 - Training loss: 0.037 - Valid loss: 0.068\n",
      "Epoch  42/60 Batch   50/77 - Training loss: 0.013 - Valid loss: 0.049\n",
      "Epoch  43/60 Batch    0/77 - Training loss: 0.021 - Valid loss: 0.052\n",
      "Epoch  43/60 Batch   50/77 - Training loss: 0.012 - Valid loss: 0.014\n",
      "Epoch  44/60 Batch    0/77 - Training loss: 0.009 - Valid loss: 0.026\n",
      "Epoch  44/60 Batch   50/77 - Training loss: 0.003 - Valid loss: 0.023\n",
      "Epoch  45/60 Batch    0/77 - Training loss: 0.014 - Valid loss: 0.034\n",
      "Epoch  45/60 Batch   50/77 - Training loss: 0.014 - Valid loss: 0.020\n",
      "Epoch  46/60 Batch    0/77 - Training loss: 0.018 - Valid loss: 0.024\n",
      "Epoch  46/60 Batch   50/77 - Training loss: 0.019 - Valid loss: 0.046\n",
      "Epoch  47/60 Batch    0/77 - Training loss: 0.021 - Valid loss: 0.030\n",
      "Epoch  47/60 Batch   50/77 - Training loss: 0.012 - Valid loss: 0.033\n",
      "Epoch  48/60 Batch    0/77 - Training loss: 0.020 - Valid loss: 0.033\n",
      "Epoch  48/60 Batch   50/77 - Training loss: 0.015 - Valid loss: 0.018\n",
      "Epoch  49/60 Batch    0/77 - Training loss: 0.012 - Valid loss: 0.017\n",
      "Epoch  49/60 Batch   50/77 - Training loss: 0.013 - Valid loss: 0.036\n",
      "Epoch  50/60 Batch    0/77 - Training loss: 0.016 - Valid loss: 0.017\n",
      "Epoch  50/60 Batch   50/77 - Training loss: 0.006 - Valid loss: 0.038\n",
      "Epoch  51/60 Batch    0/77 - Training loss: 0.013 - Valid loss: 0.006\n",
      "Epoch  51/60 Batch   50/77 - Training loss: 0.056 - Valid loss: 0.029\n",
      "Epoch  52/60 Batch    0/77 - Training loss: 0.212 - Valid loss: 0.115\n",
      "Epoch  52/60 Batch   50/77 - Training loss: 0.114 - Valid loss: 0.069\n",
      "Epoch  53/60 Batch    0/77 - Training loss: 0.153 - Valid loss: 0.135\n",
      "Epoch  53/60 Batch   50/77 - Training loss: 0.054 - Valid loss: 0.063\n",
      "Epoch  54/60 Batch    0/77 - Training loss: 0.062 - Valid loss: 0.030\n",
      "Epoch  54/60 Batch   50/77 - Training loss: 0.037 - Valid loss: 0.018\n",
      "Epoch  55/60 Batch    0/77 - Training loss: 0.076 - Valid loss: 0.012\n",
      "Epoch  55/60 Batch   50/77 - Training loss: 0.045 - Valid loss: 0.051\n",
      "Epoch  56/60 Batch    0/77 - Training loss: 0.075 - Valid loss: 0.045\n",
      "Epoch  56/60 Batch   50/77 - Training loss: 0.061 - Valid loss: 0.045\n",
      "Epoch  57/60 Batch    0/77 - Training loss: 0.064 - Valid loss: 0.033\n",
      "Epoch  57/60 Batch   50/77 - Training loss: 0.012 - Valid loss: 0.021\n",
      "Epoch  58/60 Batch    0/77 - Training loss: 0.042 - Valid loss: 0.020\n",
      "Epoch  58/60 Batch   50/77 - Training loss: 0.011 - Valid loss: 0.036\n",
      "Epoch  59/60 Batch    0/77 - Training loss: 0.046 - Valid loss: 0.030\n",
      "Epoch  59/60 Batch   50/77 - Training loss: 0.019 - Valid loss: 0.042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60/60 Batch    0/77 - Training loss: 0.022 - Valid loss: 0.053\n",
      "Epoch  60/60 Batch   50/77 - Training loss: 0.010 - Valid loss: 0.036\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "train_source = source_input_ids[batch_size:]\n",
    "train_target = target_input_ids[batch_size:]\n",
    "valid_source = source_input_ids[:batch_size]\n",
    "valid_target = target_input_ids[:batch_size]\n",
    "\n",
    "(valid_sources_batch, valid_sources_lengths, valid_targets_batch, valid_targets_lengths) = next(get_batches(\n",
    "    valid_target, valid_source, batch_size, source_char2index[\"<PAD>\"], target_char2index[\"<PAD>\"]))\n",
    "display_step = 50\n",
    "\n",
    "check_point = \"./abc.ckpt\"\n",
    "with tf.Session(graph = train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(1, epochs + 1):\n",
    "        for batch_i, (source_batch, source_lengths, target_batch, target_lengths) in enumerate(\n",
    "            get_batches(train_target, train_source, batch_size, source_char2index[\"<PAD>\"], target_char2index[\"<PAD>\"])):\n",
    "            _, loss = sess.run([train_op, cost], feed_dict = {\n",
    "                inputs:source_batch,\n",
    "                targets:target_batch,\n",
    "                target_sequence_length : target_lengths,\n",
    "                source_sequence_length : source_lengths\n",
    "            })\n",
    "            \n",
    "            if batch_i % display_step == 0:\n",
    "                valid_loss = sess.run([cost], feed_dict = {\n",
    "                    inputs: valid_sources_batch,\n",
    "                    targets: valid_targets_batch,\n",
    "                    target_sequence_length : valid_targets_lengths,\n",
    "                    source_sequence_length : valid_sources_lengths\n",
    "                })\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Training loss:{:>6.3f} - Valid loss:{:>6.3f}'\n",
    "                     .format(epoch_i, epochs,\n",
    "                            batch_i, len(train_source) // batch_size,\n",
    "                            loss, valid_loss[0]))\n",
    "            \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, check_point)\n",
    "    print('Model Trained and Saved')\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./abc.ckpt\n",
      "原始输入： common\n",
      "\n",
      "Source\n",
      " word 编号：[9, 27, 29, 29, 27, 20, 0]\n",
      " Input word:['c', 'o', 'm', 'm', 'o', 'n', '<PAD>']\n",
      "\n",
      "Target\n",
      " word 编号：[9, 29, 29, 20, 27, 27]\n",
      " output word:['c', 'm', 'm', 'n', 'o', 'o']\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "def source_to_seq(text):\n",
    "    seq_length = 7\n",
    "    return [source_char2index.get(c, source_char2index.get(\"<UNK>\")) for c in text] + [source_char2index.get(\"<PAD>\") for i in range(seq_length - len(text))]\n",
    "\n",
    "def predict_sequece(word):\n",
    "    input_word = word\n",
    "    text = source_to_seq(input_word)\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph = loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(check_point + '.meta')\n",
    "        loader.restore(sess, check_point)\n",
    "        inputdata = [text] * batch_size\n",
    "        \n",
    "        input_node = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "        logits = loaded_graph.get_tensor_by_name(\"predictions:0\")\n",
    "        source_seq_length = loaded_graph.get_tensor_by_name(\"source_sequence_length:0\")\n",
    "        target_seq_length = loaded_graph.get_tensor_by_name(\"target_sequence_length:0\")\n",
    "        \n",
    "        answer = sess.run(logits, feed_dict = {input_node :inputdata,\n",
    "                                       source_seq_length : [len(word)] * batch_size,\n",
    "                                                target_seq_length: [len(word)] * batch_size})[0]\n",
    "        \n",
    "        print(\"原始输入：\", word)\n",
    "        print(\"\\nSource\")\n",
    "        print(\" word 编号：{}\".format([x for x in text]))\n",
    "        print(\" Input word:{}\".format([source_index2char.get(x) for x in text]))\n",
    "        \n",
    "        print(\"\\nTarget\")\n",
    "        print(\" word 编号：{}\".format([x for x in answer]))\n",
    "        print(\" output word:{}\".format([target_index2char.get(x) for x in answer]))\n",
    "predict_sequece(\"common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./abc.ckpt\n",
      "原始输入： target\n",
      "\n",
      "Source\n",
      " word 编号：[25, 17, 10, 26, 6, 25, 0]\n",
      " Input word:['t', 'a', 'r', 'g', 'e', 't', '<PAD>']\n",
      "\n",
      "Target\n",
      " word 编号：[17, 6, 26, 10, 25, 25]\n",
      " output word:['a', 'e', 'g', 'r', 't', 't']\n"
     ]
    }
   ],
   "source": [
    "predict_sequece(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
