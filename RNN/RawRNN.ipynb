{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGCZ_U1PoAhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-28 15:44:35.966320\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "print(\"{}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkSCEvdv1qKV"
   },
   "source": [
    "# 下载文件，解压"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BBwFdozCnaF9"
   },
   "outputs": [],
   "source": [
    "URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "\n",
    "def generate_samples(data_dir, tmp_dir):\n",
    " imdb_dir = os.path.join(tmp_dir, \"aclImdb\")\n",
    " if not tf.gfile.Exists(imdb_dir):\n",
    "   with tarfile.open(data_dir, \"r:gz\") as tar:\n",
    "     tar.extractall(tmp_dir)\n",
    " return imdb_dir\n",
    "        \n",
    "  \n",
    "def doc_generator(imdb_dir, dataset, include_label=False):\n",
    "  dirs = [(os.path.join(imdb_dir, dataset, \"pos\"), True), (os.path.join(\n",
    "      imdb_dir, dataset, \"neg\"), False)]\n",
    "\n",
    "  for d, label in dirs:\n",
    "    for filename in os.listdir(d):\n",
    "      with tf.gfile.Open(os.path.join(d, filename)) as imdb_f:\n",
    "        doc = imdb_f.read().strip()\n",
    "        if include_label:\n",
    "          yield doc, label\n",
    "        else:\n",
    "          yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "meePrbhPoVjX",
    "outputId": "430507d1-c64c-4d55-dec7-f9a88ffff202"
   },
   "outputs": [],
   "source": [
    "imdb_file = tf.keras.utils.get_file(URL.split(\"/\")[-1], URL)\n",
    "doc_dirs = generate_samples(imdb_file, \"tmp\")\n",
    "max_sent_len = 500\n",
    "doc_label_train = [(doc[:max_sent_len].lower(), label) for doc, label in doc_generator(doc_dirs,\"train\", True)]\n",
    "doc_label_test = [(doc[:max_sent_len].lower(), label) for doc, label in doc_generator(doc_dirs,\"test\", True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fu0tJ5sg1us2"
   },
   "source": [
    "# 生成词典，token id化\n",
    "重要使用tf.keras.preprocesssing.text.Tokenizer来进行ID化，这个类资料比较少，不过直接看[源码](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py)也很容易。有个doc可以看下[Tokenizer\n",
    " 文档](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0pWT8BwrE_i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 7, 4, 0, 4]]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "class Tokenizer:\n",
    "    def __init__(self, \n",
    "                 num_words, \n",
    "                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                 split=' ',\n",
    "                 predifine_token = ['<UNK>', '<GO>', '<PAD>', '<EOS>']):\n",
    "        self.num_words = num_words\n",
    "        self.filters = filters\n",
    "        self.split = split\n",
    "        self.word_index = {}\n",
    "        self.words = []\n",
    "        self.index_word = {}\n",
    "        self.counter = collections.Counter()\n",
    "        self.predifine_token = predifine_token\n",
    "        \n",
    "        translate_dict = dict((c, split) for c in filters)\n",
    "        self.translate_map = str.maketrans(translate_dict)\n",
    "        self.words = self.predifine_token\n",
    "        \n",
    "    def __split_text(self, text):\n",
    "        parts = text.lower().replace(\"<br />\", \" \").translate(self.translate_map).split(self.split)\n",
    "        return [p for p in parts if p]\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        self.words = self.predifine_token.copy()\n",
    "        for doc in texts:\n",
    "            parts = self.__split_text(doc)\n",
    "            self.counter.update(parts)\n",
    "        \n",
    "        for word,count in self.counter.most_common(self.num_words - len(self.words)):\n",
    "            self.words.append(word)\n",
    "        # print(self.words)\n",
    "        self.word_index = {w:i for i, w in enumerate(self.words)}\n",
    "        # print(self.word_index)\n",
    "        self.index_word = dict([(i,w) for w, i in self.word_index.items()])\n",
    "\n",
    "        # print(self.index_word)\n",
    "        \n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        unknown = self.word_index.get(\"<UNK>\")\n",
    "        ret = []\n",
    "        for text in texts:\n",
    "            parts = self.__split_text(text)\n",
    "            seq = [self.word_index.get(word, unknown) for word in parts]\n",
    "            ret.append(seq)\n",
    "        return ret\n",
    "        \n",
    "\n",
    "def Test_Tokenizer():\n",
    "    token = Tokenizer(500)\n",
    "    token.fit_on_texts([\"is's is a test. Hass Test\"])\n",
    "    token.fit_on_texts([\"is's is a test. Hass2 Test\"])\n",
    "    ret = token.texts_to_sequences([\"is's is a test. sabs, test\"])\n",
    "    print(ret)\n",
    "Test_Tokenizer()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PaTbzZwR3YxL",
    "outputId": "72820566-94b2-4591-e421-dcd147ab6324"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '<UNK>'),\n",
       " (1, '<GO>'),\n",
       " (2, '<PAD>'),\n",
       " (3, '<EOS>'),\n",
       " (4, 'the'),\n",
       " (5, 'a'),\n",
       " (6, 'and'),\n",
       " (7, 'of'),\n",
       " (8, 'to'),\n",
       " (9, 'is')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 15000\n",
    "\n",
    "def init_tokenizer(docs):\n",
    "  #tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
    "  tokenizer = Tokenizer(vocab_size)\n",
    "  tokenizer.fit_on_texts(docs)\n",
    "  # 支持很多方法，可以参考源码来使用\n",
    "  # tokenizer.to_json()\n",
    "  # tokenizer.word_docs \n",
    "  # tokenizer.word_counts\n",
    "#   tokenizer.word_index.items()\n",
    "#   tokenizer.texts_to_sequences([doc_label_test[0][0]])\n",
    "  return tokenizer\n",
    "\n",
    "docs, _ = zip(*doc_label_train)\n",
    "tokenizer = init_tokenizer(docs)\n",
    "# list(tokenizer.word_index.items())\n",
    "list(tokenizer.index_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2cOWbXzaKD9"
   },
   "source": [
    "# PAD\n",
    "训练时每个语句的长短不一致，需要将句子补齐到一致的长度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKmQ0HQhxuGA"
   },
   "source": [
    "tokenizer.texts_to_sequences 用于把包含多个文本串的list，转换成对应的id化的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__UHW3L3xK_g"
   },
   "outputs": [],
   "source": [
    "def data_to_ids(doc_label, tokenizer, max_len):\n",
    "#   docs = [doc for doc, label in doc_label]\n",
    "#   labels = [label for doc, label in doc_label]\n",
    "  docs, labels = zip(*doc_label)\n",
    "  \n",
    "  print(type(docs))\n",
    "  docs_ids = tokenizer.texts_to_sequences(docs)\n",
    "  docs_ids_pad = tf.keras.preprocessing.sequence.pad_sequences(docs_ids, maxlen = max_len, \n",
    "                                              dtype=\"int32\", \n",
    "                                              padding = \"post\",\n",
    "                                             value=2)\n",
    "  labels_ids = [[1, 0] if lbl else [0, 1] for lbl in labels]\n",
    "  lengths = [len(doc) for doc in docs_ids]\n",
    "  # print(lengths)\n",
    "  return zip(docs_ids_pad, labels_ids, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "t8srlc9ywP83",
    "outputId": "f1921d75-21ad-4da0-e4a4-e0fe7174fedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_ids_train = list(data_to_ids(doc_label_train, tokenizer, max_sent_len))\n",
    "doc_ids_test = list(data_to_ids(doc_label_test, tokenizer, max_sent_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as much as i have enjoyed the hanzo the razor movies, three is definitely enough: 'who's got the gold?', the final adventure for the japanese lawman with the impressive package, is a fairly enjoyable piece of pinku cinema, but offers little new in terms of ideas whilst taking a big step backwards as far as outrageousness is concerned.<br /><br />the film opens with the appearance of a female ghost, and looks as though it is going to explore supernatural territory, something which might have take\n",
      "17 79 17 10 24 370 4 7023 4 5315 73 269 9 438 230 0 165 4 1783 846 4 731 955 21 4 882 12348 19 4 1309 5529 9 5 992 687 381 7 0 414 20 1712 117 147 12 1409 7 988 1921 700 5 178 1651 6424 17 229 17 0 9 2216 4 18 2076 19 4 1549 7 5 746 1123 6 324 17 176 13 9 168 8 3548 2533 3373 152 74 270 24 212 \n",
      "as much as i have enjoyed the hanzo the razor movies three is definitely enough <UNK> got the gold ' the final adventure for the japanese lawman with the impressive package is a fairly enjoyable piece of <UNK> cinema but offers little new in terms of ideas whilst taking a big step backwards as far as <UNK> is concerned the film opens with the appearance of a female ghost and looks as though it is going to explore supernatural territory something which might have take \n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(doc_label_train[11][0])\n",
    "xx = \"\"\n",
    "txt = \"\"\n",
    "for x in doc_ids_train[11][0]:\n",
    "    if x != 2:\n",
    "        xx += str(x) + \" \"\n",
    "        txt += tokenizer.index_word.get(x) + \" \"\n",
    "\n",
    "print(xx)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KWX0U8kycd5"
   },
   "source": [
    "# 开始定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "hidden_unit_num = 150\n",
    "batch_size = 32\n",
    "epoch_count = 50\n",
    "n_classes = 2\n",
    "eval_batch = 500\n",
    "show_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKm0PbN0yeXz"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TAjQkbh3T8k"
   },
   "source": [
    "# 第一版的实现\n",
    "不能收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "vRor0rE23TJe",
    "outputId": "8c243cc5-e51c-48b8-8fbc-26b64cd63db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-28 15:44:46.665128 \t epoch:0\t batch_index:0\t loss:0.35230016708374023\n",
      "2018-11-28 15:44:46.842621 \t epoch:0 \t batch_index:0 \t accuracy:0.6\n",
      "2018-11-28 15:45:01.899348 \t epoch:0\t batch_index:50\t loss:0.34785908460617065\n",
      "2018-11-28 15:45:17.093456 \t epoch:0\t batch_index:100\t loss:0.34630095958709717\n",
      "2018-11-28 15:45:32.678048 \t epoch:0\t batch_index:150\t loss:0.3433159589767456\n",
      "2018-11-28 15:45:48.093152 \t epoch:0\t batch_index:200\t loss:0.34873127937316895\n",
      "2018-11-28 15:46:03.630770 \t epoch:0\t batch_index:250\t loss:0.3439143896102905\n",
      "2018-11-28 15:46:18.830237 \t epoch:0\t batch_index:300\t loss:0.3478662073612213\n",
      "2018-11-28 15:46:34.646806 \t epoch:0\t batch_index:350\t loss:0.3478981852531433\n",
      "2018-11-28 15:46:50.126241 \t epoch:0\t batch_index:400\t loss:0.34836164116859436\n",
      "2018-11-28 15:47:05.593406 \t epoch:0\t batch_index:450\t loss:0.3477240800857544\n",
      "2018-11-28 15:47:20.987085 \t epoch:0\t batch_index:500\t loss:0.34738168120384216\n",
      "2018-11-28 15:47:21.137271 \t epoch:0 \t batch_index:500 \t accuracy:0.6\n",
      "2018-11-28 15:47:36.252303 \t epoch:0\t batch_index:550\t loss:0.34685447812080383\n",
      "2018-11-28 15:47:51.466832 \t epoch:0\t batch_index:600\t loss:0.3474482297897339\n",
      "2018-11-28 15:48:06.774863 \t epoch:0\t batch_index:650\t loss:0.34630048274993896\n",
      "2018-11-28 15:48:22.374448 \t epoch:0\t batch_index:700\t loss:0.3468118906021118\n",
      "2018-11-28 15:48:37.710705 \t epoch:0\t batch_index:750\t loss:0.3490388095378876\n",
      "2018-11-28 15:48:47.274429 \t epoch:1\t batch_index:0\t loss:0.3485609292984009\n",
      "2018-11-28 15:48:47.415249 \t epoch:1 \t batch_index:0 \t accuracy:0.6\n",
      "2018-11-28 15:49:03.267404 \t epoch:1\t batch_index:50\t loss:0.34919971227645874\n",
      "2018-11-28 15:49:18.487141 \t epoch:1\t batch_index:100\t loss:0.34660717844963074\n",
      "2018-11-28 15:49:33.666849 \t epoch:1\t batch_index:150\t loss:0.34686124324798584\n",
      "2018-11-28 15:49:49.006310 \t epoch:1\t batch_index:200\t loss:0.34745052456855774\n",
      "2018-11-28 15:50:04.473131 \t epoch:1\t batch_index:250\t loss:0.34868064522743225\n",
      "2018-11-28 15:50:20.203160 \t epoch:1\t batch_index:300\t loss:0.3458821177482605\n",
      "2018-11-28 15:50:35.402278 \t epoch:1\t batch_index:350\t loss:0.34719836711883545\n",
      "2018-11-28 15:50:50.803372 \t epoch:1\t batch_index:400\t loss:0.34451717138290405\n",
      "2018-11-28 15:51:06.156604 \t epoch:1\t batch_index:450\t loss:0.34640276432037354\n",
      "2018-11-28 15:51:21.472503 \t epoch:1\t batch_index:500\t loss:0.3465918004512787\n",
      "2018-11-28 15:51:21.611557 \t epoch:1 \t batch_index:500 \t accuracy:0.3\n",
      "2018-11-28 15:51:37.205335 \t epoch:1\t batch_index:550\t loss:0.346005916595459\n",
      "2018-11-28 15:51:52.696749 \t epoch:1\t batch_index:600\t loss:0.3460845351219177\n",
      "2018-11-28 15:52:08.082212 \t epoch:1\t batch_index:650\t loss:0.34806397557258606\n",
      "2018-11-28 15:52:23.445979 \t epoch:1\t batch_index:700\t loss:0.3475666046142578\n",
      "2018-11-28 15:52:38.692210 \t epoch:1\t batch_index:750\t loss:0.3467438220977783\n",
      "2018-11-28 15:52:48.028636 \t epoch:2\t batch_index:0\t loss:0.34592556953430176\n",
      "2018-11-28 15:52:48.184921 \t epoch:2 \t batch_index:0 \t accuracy:0.4\n",
      "2018-11-28 15:53:03.620286 \t epoch:2\t batch_index:50\t loss:0.34417879581451416\n",
      "2018-11-28 15:53:19.029133 \t epoch:2\t batch_index:100\t loss:0.3458004593849182\n",
      "2018-11-28 15:53:34.496882 \t epoch:2\t batch_index:150\t loss:0.3447774052619934\n",
      "2018-11-28 15:53:49.811100 \t epoch:2\t batch_index:200\t loss:0.3466530442237854\n",
      "2018-11-28 15:54:05.332473 \t epoch:2\t batch_index:250\t loss:0.34434783458709717\n",
      "2018-11-28 15:54:20.652372 \t epoch:2\t batch_index:300\t loss:0.3458747863769531\n",
      "2018-11-28 15:54:36.054205 \t epoch:2\t batch_index:350\t loss:0.3464498221874237\n",
      "2018-11-28 15:54:51.225896 \t epoch:2\t batch_index:400\t loss:0.34582507610321045\n",
      "2018-11-28 15:55:06.607296 \t epoch:2\t batch_index:450\t loss:0.3482625186443329\n",
      "2018-11-28 15:55:22.006339 \t epoch:2\t batch_index:500\t loss:0.34661316871643066\n",
      "2018-11-28 15:55:22.142971 \t epoch:2 \t batch_index:500 \t accuracy:0.8\n",
      "2018-11-28 15:55:37.369869 \t epoch:2\t batch_index:550\t loss:0.34649983048439026\n",
      "2018-11-28 15:55:52.501479 \t epoch:2\t batch_index:600\t loss:0.3463232219219208\n",
      "2018-11-28 15:56:07.497813 \t epoch:2\t batch_index:650\t loss:0.3471578061580658\n",
      "2018-11-28 15:56:23.452927 \t epoch:2\t batch_index:700\t loss:0.35130947828292847\n",
      "2018-11-28 15:56:38.724914 \t epoch:2\t batch_index:750\t loss:0.34583523869514465\n",
      "2018-11-28 15:56:48.153712 \t epoch:3\t batch_index:0\t loss:0.3466185927391052\n",
      "2018-11-28 15:56:48.307141 \t epoch:3 \t batch_index:0 \t accuracy:0.3\n",
      "2018-11-28 15:57:03.703279 \t epoch:3\t batch_index:50\t loss:0.3451339602470398\n",
      "2018-11-28 15:57:18.989527 \t epoch:3\t batch_index:100\t loss:0.3407213091850281\n",
      "2018-11-28 15:57:34.220016 \t epoch:3\t batch_index:150\t loss:0.34606850147247314\n",
      "2018-11-28 15:57:49.368440 \t epoch:3\t batch_index:200\t loss:0.3478734791278839\n",
      "2018-11-28 15:58:04.582407 \t epoch:3\t batch_index:250\t loss:0.34633809328079224\n",
      "2018-11-28 15:58:19.571085 \t epoch:3\t batch_index:300\t loss:0.3479756712913513\n",
      "2018-11-28 15:58:34.855839 \t epoch:3\t batch_index:350\t loss:0.3514707088470459\n",
      "2018-11-28 15:58:50.023598 \t epoch:3\t batch_index:400\t loss:0.35002660751342773\n",
      "2018-11-28 15:59:05.493195 \t epoch:3\t batch_index:450\t loss:0.34594810009002686\n",
      "2018-11-28 15:59:20.622622 \t epoch:3\t batch_index:500\t loss:0.3462449312210083\n",
      "2018-11-28 15:59:20.763140 \t epoch:3 \t batch_index:500 \t accuracy:0.5\n",
      "2018-11-28 15:59:35.946889 \t epoch:3\t batch_index:550\t loss:0.34680497646331787\n",
      "2018-11-28 15:59:51.003744 \t epoch:3\t batch_index:600\t loss:0.3473271429538727\n",
      "2018-11-28 16:00:05.949891 \t epoch:3\t batch_index:650\t loss:0.3462271988391876\n",
      "2018-11-28 16:00:21.102692 \t epoch:3\t batch_index:700\t loss:0.34665483236312866\n",
      "2018-11-28 16:00:35.847143 \t epoch:3\t batch_index:750\t loss:0.34658944606781006\n",
      "2018-11-28 16:00:45.136583 \t epoch:4\t batch_index:0\t loss:0.3465764820575714\n",
      "2018-11-28 16:00:45.276030 \t epoch:4 \t batch_index:0 \t accuracy:0.6\n",
      "2018-11-28 16:01:00.259264 \t epoch:4\t batch_index:50\t loss:0.3458932042121887\n",
      "2018-11-28 16:01:15.373766 \t epoch:4\t batch_index:100\t loss:0.34685811400413513\n",
      "2018-11-28 16:01:30.298977 \t epoch:4\t batch_index:150\t loss:0.34688031673431396\n",
      "2018-11-28 16:01:45.388908 \t epoch:4\t batch_index:200\t loss:0.3472495377063751\n",
      "2018-11-28 16:02:00.362972 \t epoch:4\t batch_index:250\t loss:0.3456382751464844\n",
      "2018-11-28 16:02:15.284305 \t epoch:4\t batch_index:300\t loss:0.34641024470329285\n",
      "2018-11-28 16:02:29.994010 \t epoch:4\t batch_index:350\t loss:0.3469644784927368\n",
      "2018-11-28 16:02:45.158939 \t epoch:4\t batch_index:400\t loss:0.3453153371810913\n",
      "2018-11-28 16:03:00.260097 \t epoch:4\t batch_index:450\t loss:0.34163036942481995\n",
      "2018-11-28 16:03:15.222089 \t epoch:4\t batch_index:500\t loss:0.34531140327453613\n",
      "2018-11-28 16:03:15.369598 \t epoch:4 \t batch_index:500 \t accuracy:0.2\n",
      "2018-11-28 16:03:30.286127 \t epoch:4\t batch_index:550\t loss:0.3466338515281677\n",
      "2018-11-28 16:03:45.129963 \t epoch:4\t batch_index:600\t loss:0.34552714228630066\n",
      "2018-11-28 16:03:59.673942 \t epoch:4\t batch_index:650\t loss:0.3466866612434387\n",
      "2018-11-28 16:04:14.486765 \t epoch:4\t batch_index:700\t loss:0.3471727967262268\n",
      "2018-11-28 16:04:29.426960 \t epoch:4\t batch_index:750\t loss:0.3458765149116516\n",
      "2018-11-28 16:04:38.790151 \t epoch:5\t batch_index:0\t loss:0.34673207998275757\n",
      "2018-11-28 16:04:38.942157 \t epoch:5 \t batch_index:0 \t accuracy:0.7\n",
      "2018-11-28 16:04:54.053287 \t epoch:5\t batch_index:50\t loss:0.34746214747428894\n",
      "2018-11-28 16:05:09.393050 \t epoch:5\t batch_index:100\t loss:0.3470463156700134\n",
      "2018-11-28 16:05:24.945296 \t epoch:5\t batch_index:150\t loss:0.3466140329837799\n",
      "2018-11-28 16:05:40.114625 \t epoch:5\t batch_index:200\t loss:0.34633272886276245\n",
      "2018-11-28 16:05:54.924203 \t epoch:5\t batch_index:250\t loss:0.3494333326816559\n",
      "2018-11-28 16:06:10.025831 \t epoch:5\t batch_index:300\t loss:0.34567081928253174\n",
      "2018-11-28 16:06:25.019503 \t epoch:5\t batch_index:350\t loss:0.3456203043460846\n",
      "2018-11-28 16:06:40.136923 \t epoch:5\t batch_index:400\t loss:0.3465768098831177\n",
      "2018-11-28 16:06:55.258269 \t epoch:5\t batch_index:450\t loss:0.3465093970298767\n",
      "2018-11-28 16:07:10.331301 \t epoch:5\t batch_index:500\t loss:0.34625476598739624\n",
      "2018-11-28 16:07:10.480712 \t epoch:5 \t batch_index:500 \t accuracy:0.4\n",
      "2018-11-28 16:07:25.584993 \t epoch:5\t batch_index:550\t loss:0.34657400846481323\n",
      "2018-11-28 16:07:40.666436 \t epoch:5\t batch_index:600\t loss:0.3469294607639313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-28 16:07:55.648590 \t epoch:5\t batch_index:650\t loss:0.3462136685848236\n",
      "2018-11-28 16:08:10.856084 \t epoch:5\t batch_index:700\t loss:0.3464123606681824\n",
      "2018-11-28 16:08:25.850326 \t epoch:5\t batch_index:750\t loss:0.34554994106292725\n",
      "2018-11-28 16:08:35.157023 \t epoch:6\t batch_index:0\t loss:0.34676626324653625\n",
      "2018-11-28 16:08:35.306441 \t epoch:6 \t batch_index:0 \t accuracy:0.3\n",
      "2018-11-28 16:08:50.586829 \t epoch:6\t batch_index:50\t loss:0.3465060591697693\n",
      "2018-11-28 16:09:05.558919 \t epoch:6\t batch_index:100\t loss:0.3470291495323181\n",
      "2018-11-28 16:09:20.800670 \t epoch:6\t batch_index:150\t loss:0.3464221954345703\n",
      "2018-11-28 16:09:35.858120 \t epoch:6\t batch_index:200\t loss:0.34614482522010803\n",
      "2018-11-28 16:09:50.831974 \t epoch:6\t batch_index:250\t loss:0.3495182991027832\n",
      "2018-11-28 16:10:06.021419 \t epoch:6\t batch_index:300\t loss:0.34661054611206055\n",
      "2018-11-28 16:10:21.214183 \t epoch:6\t batch_index:350\t loss:0.34819406270980835\n",
      "2018-11-28 16:10:36.255597 \t epoch:6\t batch_index:400\t loss:0.34460294246673584\n",
      "2018-11-28 16:10:51.835326 \t epoch:6\t batch_index:450\t loss:0.34647268056869507\n",
      "2018-11-28 16:11:06.788399 \t epoch:6\t batch_index:500\t loss:0.348733127117157\n",
      "2018-11-28 16:11:06.925818 \t epoch:6 \t batch_index:500 \t accuracy:0.5\n",
      "2018-11-28 16:11:22.217079 \t epoch:6\t batch_index:550\t loss:0.345855712890625\n",
      "2018-11-28 16:11:36.889485 \t epoch:6\t batch_index:600\t loss:0.34669792652130127\n",
      "2018-11-28 16:11:51.505248 \t epoch:6\t batch_index:650\t loss:0.34681499004364014\n",
      "2018-11-28 16:12:06.361371 \t epoch:6\t batch_index:700\t loss:0.34643763303756714\n",
      "2018-11-28 16:12:21.332555 \t epoch:6\t batch_index:750\t loss:0.34679070115089417\n",
      "2018-11-28 16:12:30.770534 \t epoch:7\t batch_index:0\t loss:0.34667354822158813\n",
      "2018-11-28 16:12:30.909914 \t epoch:7 \t batch_index:0 \t accuracy:0.4\n",
      "2018-11-28 16:12:46.417856 \t epoch:7\t batch_index:50\t loss:0.34658318758010864\n",
      "2018-11-28 16:13:01.534877 \t epoch:7\t batch_index:100\t loss:0.34536123275756836\n",
      "2018-11-28 16:13:17.360330 \t epoch:7\t batch_index:150\t loss:0.3475983738899231\n",
      "2018-11-28 16:13:32.453436 \t epoch:7\t batch_index:200\t loss:0.3462657034397125\n",
      "2018-11-28 16:13:47.934631 \t epoch:7\t batch_index:250\t loss:0.3466986417770386\n",
      "2018-11-28 16:14:02.819335 \t epoch:7\t batch_index:300\t loss:0.3466590642929077\n",
      "2018-11-28 16:14:17.798650 \t epoch:7\t batch_index:350\t loss:0.3454732298851013\n",
      "2018-11-28 16:14:33.333261 \t epoch:7\t batch_index:400\t loss:0.3471115231513977\n",
      "2018-11-28 16:14:48.542246 \t epoch:7\t batch_index:450\t loss:0.34754592180252075\n",
      "2018-11-28 16:15:03.918561 \t epoch:7\t batch_index:500\t loss:0.34687256813049316\n",
      "2018-11-28 16:15:04.065388 \t epoch:7 \t batch_index:500 \t accuracy:0.6\n",
      "2018-11-28 16:15:19.446129 \t epoch:7\t batch_index:550\t loss:0.34661775827407837\n",
      "2018-11-28 16:15:34.195194 \t epoch:7\t batch_index:600\t loss:0.34632229804992676\n",
      "2018-11-28 16:15:49.254900 \t epoch:7\t batch_index:650\t loss:0.34658950567245483\n",
      "2018-11-28 16:16:04.337949 \t epoch:7\t batch_index:700\t loss:0.3466035723686218\n",
      "2018-11-28 16:16:19.456260 \t epoch:7\t batch_index:750\t loss:0.3471689820289612\n",
      "2018-11-28 16:16:28.832522 \t epoch:8\t batch_index:0\t loss:0.3463377058506012\n",
      "2018-11-28 16:16:28.970513 \t epoch:8 \t batch_index:0 \t accuracy:0.6\n",
      "2018-11-28 16:16:43.777209 \t epoch:8\t batch_index:50\t loss:0.3423658311367035\n",
      "2018-11-28 16:16:58.632939 \t epoch:8\t batch_index:100\t loss:0.3456650376319885\n",
      "2018-11-28 16:17:13.918887 \t epoch:8\t batch_index:150\t loss:0.34837573766708374\n",
      "2018-11-28 16:17:29.066576 \t epoch:8\t batch_index:200\t loss:0.3484104871749878\n",
      "2018-11-28 16:17:44.089104 \t epoch:8\t batch_index:250\t loss:0.34489601850509644\n",
      "2018-11-28 16:17:59.272992 \t epoch:8\t batch_index:300\t loss:0.34620365500450134\n",
      "2018-11-28 16:18:14.634273 \t epoch:8\t batch_index:350\t loss:0.34638574719429016\n",
      "2018-11-28 16:18:29.757457 \t epoch:8\t batch_index:400\t loss:0.3465120196342468\n",
      "2018-11-28 16:18:44.858115 \t epoch:8\t batch_index:450\t loss:0.3464497923851013\n",
      "2018-11-28 16:19:00.113323 \t epoch:8\t batch_index:500\t loss:0.34652313590049744\n",
      "2018-11-28 16:19:00.255019 \t epoch:8 \t batch_index:500 \t accuracy:0.4\n",
      "2018-11-28 16:19:15.362213 \t epoch:8\t batch_index:550\t loss:0.34657394886016846\n",
      "2018-11-28 16:19:30.203376 \t epoch:8\t batch_index:600\t loss:0.3470238149166107\n",
      "2018-11-28 16:19:45.054163 \t epoch:8\t batch_index:650\t loss:0.3465917706489563\n",
      "2018-11-28 16:20:00.246511 \t epoch:8\t batch_index:700\t loss:0.3471192419528961\n",
      "2018-11-28 16:20:15.469176 \t epoch:8\t batch_index:750\t loss:0.34649229049682617\n",
      "2018-11-28 16:20:24.733320 \t epoch:9\t batch_index:0\t loss:0.3468755781650543\n",
      "2018-11-28 16:20:24.908205 \t epoch:9 \t batch_index:0 \t accuracy:0.5\n",
      "2018-11-28 16:20:39.780479 \t epoch:9\t batch_index:50\t loss:0.34582632780075073\n",
      "2018-11-28 16:20:54.763550 \t epoch:9\t batch_index:100\t loss:0.34686246514320374\n",
      "2018-11-28 16:21:09.813773 \t epoch:9\t batch_index:150\t loss:0.34678909182548523\n",
      "2018-11-28 16:21:24.622440 \t epoch:9\t batch_index:200\t loss:0.3465559482574463\n",
      "2018-11-28 16:21:39.875765 \t epoch:9\t batch_index:250\t loss:0.34685876965522766\n",
      "2018-11-28 16:21:55.121867 \t epoch:9\t batch_index:300\t loss:0.34616607427597046\n",
      "2018-11-28 16:22:10.442036 \t epoch:9\t batch_index:350\t loss:0.3476676940917969\n",
      "2018-11-28 16:22:25.686801 \t epoch:9\t batch_index:400\t loss:0.3470924496650696\n",
      "2018-11-28 16:22:40.441957 \t epoch:9\t batch_index:450\t loss:0.34531593322753906\n",
      "2018-11-28 16:22:55.739163 \t epoch:9\t batch_index:500\t loss:0.34600043296813965\n",
      "2018-11-28 16:22:55.887611 \t epoch:9 \t batch_index:500 \t accuracy:0.3\n",
      "2018-11-28 16:23:11.311867 \t epoch:9\t batch_index:550\t loss:0.34637776017189026\n",
      "2018-11-28 16:23:26.559051 \t epoch:9\t batch_index:600\t loss:0.34660956263542175\n",
      "2018-11-28 16:23:41.385397 \t epoch:9\t batch_index:650\t loss:0.3459134101867676\n",
      "2018-11-28 16:23:56.566622 \t epoch:9\t batch_index:700\t loss:0.3469390571117401\n",
      "2018-11-28 16:24:12.021060 \t epoch:9\t batch_index:750\t loss:0.34659212827682495\n",
      "2018-11-28 16:24:21.273974 \t epoch:10\t batch_index:0\t loss:0.34660249948501587\n",
      "2018-11-28 16:24:21.412886 \t epoch:10 \t batch_index:0 \t accuracy:0.5\n",
      "2018-11-28 16:24:36.499011 \t epoch:10\t batch_index:50\t loss:0.3469366729259491\n",
      "2018-11-28 16:24:51.170701 \t epoch:10\t batch_index:100\t loss:0.3471221923828125\n",
      "2018-11-28 16:25:06.462415 \t epoch:10\t batch_index:150\t loss:0.3472694754600525\n",
      "2018-11-28 16:25:21.524649 \t epoch:10\t batch_index:200\t loss:0.345866322517395\n",
      "2018-11-28 16:25:36.766284 \t epoch:10\t batch_index:250\t loss:0.3480262756347656\n",
      "2018-11-28 16:25:51.887274 \t epoch:10\t batch_index:300\t loss:0.34762120246887207\n",
      "2018-11-28 16:26:06.906736 \t epoch:10\t batch_index:350\t loss:0.34680473804473877\n",
      "2018-11-28 16:26:21.973580 \t epoch:10\t batch_index:400\t loss:0.3465210199356079\n",
      "2018-11-28 16:26:37.261615 \t epoch:10\t batch_index:450\t loss:0.34597665071487427\n",
      "2018-11-28 16:26:52.371205 \t epoch:10\t batch_index:500\t loss:0.34833014011383057\n",
      "2018-11-28 16:26:52.519302 \t epoch:10 \t batch_index:500 \t accuracy:0.6\n",
      "2018-11-28 16:27:07.902733 \t epoch:10\t batch_index:550\t loss:0.3479476571083069\n",
      "2018-11-28 16:27:23.202818 \t epoch:10\t batch_index:600\t loss:0.346009224653244\n",
      "2018-11-28 16:27:38.345202 \t epoch:10\t batch_index:650\t loss:0.3475048542022705\n",
      "2018-11-28 16:27:54.022798 \t epoch:10\t batch_index:700\t loss:0.3465741276741028\n",
      "2018-11-28 16:28:09.426944 \t epoch:10\t batch_index:750\t loss:0.3466489017009735\n",
      "2018-11-28 16:28:18.927674 \t epoch:11\t batch_index:0\t loss:0.3473084568977356\n",
      "2018-11-28 16:28:19.059316 \t epoch:11 \t batch_index:0 \t accuracy:0.4\n",
      "2018-11-28 16:28:33.909954 \t epoch:11\t batch_index:50\t loss:0.3462309241294861\n",
      "2018-11-28 16:28:48.876789 \t epoch:11\t batch_index:100\t loss:0.34859365224838257\n",
      "2018-11-28 16:29:03.898249 \t epoch:11\t batch_index:150\t loss:0.346758097410202\n",
      "2018-11-28 16:29:19.097659 \t epoch:11\t batch_index:200\t loss:0.34657537937164307\n",
      "2018-11-28 16:29:34.263874 \t epoch:11\t batch_index:250\t loss:0.34693437814712524\n",
      "2018-11-28 16:29:49.069194 \t epoch:11\t batch_index:300\t loss:0.3469047248363495\n",
      "2018-11-28 16:30:04.262380 \t epoch:11\t batch_index:350\t loss:0.3465557098388672\n",
      "2018-11-28 16:30:19.235415 \t epoch:11\t batch_index:400\t loss:0.34631192684173584\n",
      "2018-11-28 16:30:34.032209 \t epoch:11\t batch_index:450\t loss:0.345694363117218\n",
      "2018-11-28 16:30:49.084301 \t epoch:11\t batch_index:500\t loss:0.34675419330596924\n",
      "2018-11-28 16:30:49.237711 \t epoch:11 \t batch_index:500 \t accuracy:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-28 16:31:04.597959 \t epoch:11\t batch_index:550\t loss:0.34743890166282654\n",
      "2018-11-28 16:31:19.721689 \t epoch:11\t batch_index:600\t loss:0.34628498554229736\n",
      "2018-11-28 16:31:35.112716 \t epoch:11\t batch_index:650\t loss:0.34618765115737915\n",
      "2018-11-28 16:31:50.491794 \t epoch:11\t batch_index:700\t loss:0.3462604582309723\n",
      "2018-11-28 16:32:05.739168 \t epoch:11\t batch_index:750\t loss:0.34633582830429077\n",
      "2018-11-28 16:32:15.075721 \t epoch:12\t batch_index:0\t loss:0.3466365337371826\n",
      "2018-11-28 16:32:15.217592 \t epoch:12 \t batch_index:0 \t accuracy:0.5\n",
      "2018-11-28 16:32:30.516915 \t epoch:12\t batch_index:50\t loss:0.3508475720882416\n",
      "2018-11-28 16:32:45.806698 \t epoch:12\t batch_index:100\t loss:0.347635418176651\n",
      "2018-11-28 16:33:01.007105 \t epoch:12\t batch_index:150\t loss:0.3465837240219116\n",
      "2018-11-28 16:33:16.030298 \t epoch:12\t batch_index:200\t loss:0.34658390283584595\n",
      "2018-11-28 16:33:31.359286 \t epoch:12\t batch_index:250\t loss:0.3462586998939514\n",
      "2018-11-28 16:33:46.514503 \t epoch:12\t batch_index:300\t loss:0.3456772565841675\n",
      "2018-11-28 16:34:01.857756 \t epoch:12\t batch_index:350\t loss:0.34617191553115845\n",
      "2018-11-28 16:34:17.167993 \t epoch:12\t batch_index:400\t loss:0.34619858860969543\n",
      "2018-11-28 16:34:32.172344 \t epoch:12\t batch_index:450\t loss:0.3466545343399048\n",
      "2018-11-28 16:34:47.217517 \t epoch:12\t batch_index:500\t loss:0.3464052379131317\n",
      "2018-11-28 16:34:47.355633 \t epoch:12 \t batch_index:500 \t accuracy:0.3\n",
      "2018-11-28 16:35:02.218420 \t epoch:12\t batch_index:550\t loss:0.3466989994049072\n",
      "2018-11-28 16:35:17.009693 \t epoch:12\t batch_index:600\t loss:0.34665146470069885\n",
      "2018-11-28 16:35:31.725164 \t epoch:12\t batch_index:650\t loss:0.34615981578826904\n",
      "2018-11-28 16:35:46.365037 \t epoch:12\t batch_index:700\t loss:0.34437721967697144\n",
      "2018-11-28 16:36:01.378493 \t epoch:12\t batch_index:750\t loss:0.34748029708862305\n",
      "2018-11-28 16:36:11.031695 \t epoch:13\t batch_index:0\t loss:0.34643077850341797\n",
      "2018-11-28 16:36:11.192668 \t epoch:13 \t batch_index:0 \t accuracy:0.4\n",
      "2018-11-28 16:36:26.494769 \t epoch:13\t batch_index:50\t loss:0.3458966612815857\n",
      "2018-11-28 16:36:41.811986 \t epoch:13\t batch_index:100\t loss:0.34683459997177124\n",
      "2018-11-28 16:36:56.918400 \t epoch:13\t batch_index:150\t loss:0.3484475314617157\n",
      "2018-11-28 16:37:12.099013 \t epoch:13\t batch_index:200\t loss:0.34368962049484253\n",
      "2018-11-28 16:37:27.437886 \t epoch:13\t batch_index:250\t loss:0.34727394580841064\n",
      "2018-11-28 16:37:42.847416 \t epoch:13\t batch_index:300\t loss:0.3464741110801697\n",
      "2018-11-28 16:37:58.424438 \t epoch:13\t batch_index:350\t loss:0.34657615423202515\n",
      "2018-11-28 16:38:13.858441 \t epoch:13\t batch_index:400\t loss:0.3458808660507202\n",
      "2018-11-28 16:38:29.233950 \t epoch:13\t batch_index:450\t loss:0.3465985655784607\n",
      "2018-11-28 16:38:44.450702 \t epoch:13\t batch_index:500\t loss:0.34664952754974365\n",
      "2018-11-28 16:38:44.581556 \t epoch:13 \t batch_index:500 \t accuracy:0.5\n",
      "2018-11-28 16:38:59.920198 \t epoch:13\t batch_index:550\t loss:0.34671783447265625\n",
      "2018-11-28 16:39:15.270973 \t epoch:13\t batch_index:600\t loss:0.34592360258102417\n",
      "2018-11-28 16:39:30.036806 \t epoch:13\t batch_index:650\t loss:0.34670111536979675\n",
      "2018-11-28 16:39:45.566297 \t epoch:13\t batch_index:700\t loss:0.34563589096069336\n",
      "2018-11-28 16:40:00.999841 \t epoch:13\t batch_index:750\t loss:0.3478897213935852\n",
      "2018-11-28 16:40:10.402471 \t epoch:14\t batch_index:0\t loss:0.3487207293510437\n",
      "2018-11-28 16:40:10.543514 \t epoch:14 \t batch_index:0 \t accuracy:0.6\n",
      "2018-11-28 16:40:25.703982 \t epoch:14\t batch_index:50\t loss:0.34664374589920044\n",
      "2018-11-28 16:40:40.714105 \t epoch:14\t batch_index:100\t loss:0.34557992219924927\n",
      "2018-11-28 16:40:56.155306 \t epoch:14\t batch_index:150\t loss:0.34669941663742065\n",
      "2018-11-28 16:41:11.512299 \t epoch:14\t batch_index:200\t loss:0.34657400846481323\n",
      "2018-11-28 16:41:26.509606 \t epoch:14\t batch_index:250\t loss:0.34829673171043396\n",
      "2018-11-28 16:41:41.719810 \t epoch:14\t batch_index:300\t loss:0.34659093618392944\n",
      "2018-11-28 16:41:57.171292 \t epoch:14\t batch_index:350\t loss:0.3464464545249939\n",
      "2018-11-28 16:42:12.250282 \t epoch:14\t batch_index:400\t loss:0.3459033668041229\n",
      "2018-11-28 16:42:27.444261 \t epoch:14\t batch_index:450\t loss:0.3463718593120575\n",
      "2018-11-28 16:42:42.821714 \t epoch:14\t batch_index:500\t loss:0.3468741178512573\n",
      "2018-11-28 16:42:42.974107 \t epoch:14 \t batch_index:500 \t accuracy:0.6\n",
      "2018-11-28 16:42:58.438046 \t epoch:14\t batch_index:550\t loss:0.34639403223991394\n",
      "2018-11-28 16:43:13.596808 \t epoch:14\t batch_index:600\t loss:0.3465813398361206\n",
      "2018-11-28 16:43:29.105152 \t epoch:14\t batch_index:650\t loss:0.34493064880371094\n",
      "2018-11-28 16:43:44.277523 \t epoch:14\t batch_index:700\t loss:0.34605497121810913\n",
      "2018-11-28 16:43:59.655146 \t epoch:14\t batch_index:750\t loss:0.3467758297920227\n",
      "2018-11-28 16:44:09.174060 \t epoch:15\t batch_index:0\t loss:0.3466654121875763\n",
      "2018-11-28 16:44:09.306868 \t epoch:15 \t batch_index:0 \t accuracy:0.4\n",
      "2018-11-28 16:44:24.318006 \t epoch:15\t batch_index:50\t loss:0.3452088534832001\n",
      "2018-11-28 16:44:39.147578 \t epoch:15\t batch_index:100\t loss:0.3463912010192871\n",
      "2018-11-28 16:44:54.380702 \t epoch:15\t batch_index:150\t loss:0.3461964726448059\n",
      "2018-11-28 16:45:09.839554 \t epoch:15\t batch_index:200\t loss:0.3462068438529968\n",
      "2018-11-28 16:45:24.978064 \t epoch:15\t batch_index:250\t loss:0.34771275520324707\n",
      "2018-11-28 16:45:40.309629 \t epoch:15\t batch_index:300\t loss:0.34673619270324707\n",
      "2018-11-28 16:45:55.554403 \t epoch:15\t batch_index:350\t loss:0.34635913372039795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputX = tf.placeholder(tf.int32, shape = [None, max_sent_len])\n",
    "    inputY = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    embeddings = tf.get_variable(\"word_embeddings\", [vocab_size, embedding_size])\n",
    "    data_embedding = tf.nn.embedding_lookup(embeddings, inputX)\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(hidden_unit_num)\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(lstm, data_embedding, dtype=tf.float32)\n",
    "    FC_W = tf.get_variable(\"fc_w\", shape=[hidden_unit_num, n_classes])\n",
    "    FC_B = tf.get_variable(\"fc_b\", shape=[n_classes])\n",
    "    logits = tf.sigmoid(tf.matmul(rnn_state.h, FC_W) + FC_B)\n",
    "    model_Y = tf.nn.softmax(logits)\n",
    "    loss = -tf.reduce_mean(inputY * tf.log(model_Y))\n",
    "    optimizer = tf.train.AdagradOptimizer(0.3)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    predict_ret = tf.argmax(model_Y, axis=1)\n",
    "    true_ret = tf.argmax(inputY, axis = 1)\n",
    "#     print(predict_ret)\n",
    "#     print(true_ret)\n",
    "#     int_ret = (predict_ret == true_ret)\n",
    "    accurcy, up_op = tf.metrics.accuracy(true_ret, predict_ret)\n",
    "\n",
    "def train_raw(tfgraph, batch_size, epoch_count, doc_ids_train, doc_ids_test):\n",
    "    train_data_len = len(doc_ids_train)\n",
    "    batch_count = train_data_len//batch_size\n",
    "    sess = tf.Session(graph = tfgraph)\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(epoch_count):\n",
    "      random.shuffle(doc_ids_train)\n",
    "      for batch_index in range(batch_count):\n",
    "        start = batch_index * batch_size\n",
    "        inX, inY, xlen = zip(*doc_ids_train[start:start+batch_size])\n",
    "    #     inX = [np.array(x) for x in inX]\n",
    "    #     inY = [np.array(y) for y in inY]\n",
    "        # print(batch_index)\n",
    "        _, ret = sess.run([train_op, loss], feed_dict={inputX:inX, inputY:inY})\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "          print(\"{} \\t epoch:{}\\t batch_index:{}\\t loss:{}\".format(datetime.datetime.now(),epoch, batch_index, ret))\n",
    "        \n",
    "        if batch_index % 500 == 0:\n",
    "            random.shuffle(doc_ids_test)\n",
    "            test_data = doc_ids_test[:10]\n",
    "            test_inX, test_inY, test_xlen = zip(*test_data)\n",
    "            py = sess.run([predict_ret], feed_dict = {inputX:test_inX})\n",
    "            y = np.argmax(test_inY, 1)\n",
    "            accuracy = np.sum(py == y) / len(y)\n",
    "            print(\"{} \\t epoch:{} \\t batch_index:{} \\t accuracy:{}\".format(datetime.datetime.now(),epoch, batch_index, accuracy)) \n",
    "\n",
    "train_raw(graph, batch_size, epoch_count, doc_ids_train, doc_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二版的实现\n",
    "问题：\n",
    "1. 训练收敛太慢\n",
    "1. 不收敛\n",
    "\n",
    "改动\n",
    "1. 增加了dynamic_rnn的sequence_length参数\n",
    "1. FC层后的loss采用tf带的函数，不用自己写的\n",
    "1. 准确率的计算放在graph中计算\n",
    "1. （可能）vocab的大小改为12000， max——sent_len改为500，提升速度\n",
    "\n",
    "上面的代码在运行时是不能收敛的，从[RNNs in Tensorflow, a Practical Guide and Undocumented Features](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/) 有一些dynamic_rnn的使用注意，我们可以修正下，在看看.\n",
    "* 加了之后没有反应呀 * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seqlength = tf.Graph()\n",
    "with graph_seqlength.as_default():\n",
    "    inputX = tf.placeholder(tf.int32, shape = [None, max_sent_len])\n",
    "    inputY = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    input_xlen = tf.placeholder(tf.int32, shape=[None])\n",
    "    embeddings = tf.get_variable(\"word_embeddings\", [vocab_size, embedding_size])\n",
    "    data_embedding = tf.nn.embedding_lookup(embeddings, inputX)\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(hidden_unit_num)\n",
    "    # initial_state = lstm.zero_state(batch_size, tf.float32)\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(lstm, data_embedding, dtype=tf.float32,\n",
    "                                             sequence_length = input_xlen)\n",
    "    #,                                         initial_state = initial_state)\n",
    "    FC_W = tf.get_variable(\"fc_w\", shape=[hidden_unit_num, n_classes])\n",
    "    FC_B = tf.get_variable(\"fc_b\", shape=[n_classes])\n",
    "    \n",
    "    pred = tf.matmul(rnn_state.h, FC_W) + FC_B\n",
    "    # logits = tf.sigmoid(pred)\n",
    "    # 为什么不用sigmoid？\n",
    "    # 不收敛，换了下loss函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=inputY))\n",
    "    \n",
    "    # model_Y = tf.nn.softmax(logits)\n",
    "    # loss = -tf.reduce_mean(inputY * tf.log(model_Y))\n",
    "    optimizer = tf.train.AdagradOptimizer(0.2).minimize(loss)\n",
    "    # train_op = optimizer\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    correctPred = tf.equal(tf.argmax(pred, axis=1), tf.argmax(inputY, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "def train_graph_seqlength(tfgraph, batch_size, epoch_count, doc_ids_train, doc_ids_test):\n",
    "    train_data_len = len(doc_ids_train)\n",
    "    batch_count = train_data_len//batch_size\n",
    "    sess = tf.Session(graph = tfgraph)\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(epoch_count):\n",
    "      random.shuffle(doc_ids_train)\n",
    "      for batch_index in range(batch_count):\n",
    "        start = batch_index * batch_size\n",
    "        inX, inY, xlen = zip(*doc_ids_train[start:start+batch_size])\n",
    "        _, ret,accu = sess.run([optimizer, loss, accuracy], feed_dict={inputX:inX, inputY:inY, input_xlen:xlen})\n",
    "        # print(st.h[:,1])\n",
    "\n",
    "        if batch_index % show_batch == 0:\n",
    "          print(\"{} \\t epoch:{}\\t batch_index:{}\\t loss:{} \\t accuracy:{}\".format(datetime.datetime.now(),epoch, batch_index, ret, accu))\n",
    "        \n",
    "        if batch_index % eval_batch == 0:\n",
    "            random.shuffle(doc_ids_test)\n",
    "            arr_accu = []\n",
    "            for test_index in range(len(doc_ids_test) // batch_size):\n",
    "                test_data = doc_ids_test[test_index:test_index + batch_size]\n",
    "                test_inX, test_inY, test_xlen = zip(*test_data)\n",
    "                accu = sess.run([accuracy], feed_dict = {inputX:test_inX, inputY:test_inY, input_xlen: test_xlen})\n",
    "                arr_accu.append(accu)\n",
    "            print(\"{} \\t epoch:{} \\t batch_index:{} \\t accuracy:{}\".format(datetime.datetime.now(),epoch, batch_index, np.mean(arr_accu)))\n",
    "\n",
    "            \n",
    "\n",
    "   \n",
    "\n",
    "train_graph_seqlength(graph_seqlength, batch_size, epoch_count, doc_ids_train, doc_ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三版\n",
    "第二版的问题：\n",
    "1. 收敛太慢\n",
    "1. 测试集上的准确率在80%左右，训练集100%，有过拟合嫌疑，可以通过优化提升\n",
    "1. 网络结构上可以在调整下，加深深度和改变结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_v3 = tf.Graph()\n",
    "with graph_v3.as_default():\n",
    "    inputX = tf.placeholder(tf.int32, shape = [None, max_sent_len])\n",
    "    inputY = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    input_xlen = tf.placeholder(tf.int32, shape=[None])\n",
    "    embeddings = tf.get_variable(\"word_embeddings\", [vocab_size, embedding_size])\n",
    "    data_embedding = tf.nn.embedding_lookup(embeddings, inputX)\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(hidden_unit_num)\n",
    "    dropcell = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob = 0.75, state_keep_prob = 0.75)\n",
    "    rnn_output, rnn_state = tf.nn.dynamic_rnn(dropcell, data_embedding, dtype=tf.float32,\n",
    "                                             sequence_length = input_xlen)\n",
    "    FC_W = tf.get_variable(\"fc_w\", shape=[hidden_unit_num, n_classes])\n",
    "    FC_B = tf.get_variable(\"fc_b\", shape=[n_classes])\n",
    "    \n",
    "    pred = tf.matmul(rnn_state.h, FC_W) + FC_B\n",
    "    # logits = tf.sigmoid(pred)\n",
    "    # 为什么不用sigmoid？\n",
    "    # 不收敛，换了下loss函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=inputY))\n",
    "    \n",
    "    # model_Y = tf.nn.softmax(logits)\n",
    "    # loss = -tf.reduce_mean(inputY * tf.log(model_Y))\n",
    "    optimizer = tf.train.AdagradOptimizer(0.2).minimize(loss)\n",
    "    # train_op = optimizer\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    correctPred = tf.equal(tf.argmax(pred, axis=1), tf.argmax(inputY, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "train_graph_seqlength(graph_v3, batch_size, epoch_count, doc_ids_train, doc_ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "* [Perform sentiment analysis with LSTMs, using TensorFlow](https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow)\n",
    "* [一个dynamic_rnn的实现](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py)\n",
    "* [RNNs in Tensorflow, a Practical Guide and Undocumented Features](https://github.com/dennybritz/tf-rnn)\n",
    "* 为什么不能用自己的交叉墒实现? 因为自己的softmax实现可能会有数据的溢出问题，np.exp(710)是一个inf [https://github.com/AliAbbasi/Numerically-Stable-Cross-Entropy-Loss-Function-Tensorflow](https://github.com/AliAbbasi/Numerically-Stable-Cross-Entropy-Loss-Function-Tensorflow)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "colab": {
   "collapsed_sections": [],
   "name": "RawRNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
